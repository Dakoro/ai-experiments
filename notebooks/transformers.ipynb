{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d2f1c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRANSFORMER ARCHITECTURE DEMONSTRATION ===\n",
      "\n",
      "1. INPUT EMBEDDING AND POSITIONAL ENCODING\n",
      "--------------------------------------------------\n",
      "Token IDs: [101 253  37 102]\n",
      "Embedding shape: (4, 64)\n",
      "After adding positional encoding: (4, 64)\n",
      "\n",
      "2. SELF-ATTENTION MECHANISM\n",
      "--------------------------------------------------\n",
      "Query (Q) shape: (4, 4)\n",
      "Key (K) shape: (4, 4)\n",
      "Value (V) shape: (4, 4)\n",
      "\n",
      "Attention scores (QK^T/√d_k):\n",
      "[[ 0.22537311 -0.51454192  0.63765767  0.37202592]\n",
      " [ 0.41856289  1.63407252 -0.05978984  0.25628838]\n",
      " [-0.4239514   0.27390864 -0.64142233 -2.16428676]\n",
      " [-0.61971087  0.6148871  -1.88436126 -1.83555439]]\n",
      "\n",
      "Attention weights (after softmax):\n",
      "[[0.24123296 0.11510538 0.36432549 0.27933616]\n",
      " [0.17117357 0.57719935 0.10609398 0.1455331 ]\n",
      " [0.25066046 0.50368915 0.20166912 0.04398127]\n",
      " [0.19937022 0.68523473 0.05628979 0.05910526]]\n",
      "Row sums (should be 1): [1. 1. 1. 1.]\n",
      "\n",
      "Attention output shape: (4, 4)\n",
      "\n",
      "3. CAUSAL MASKING (for Decoder)\n",
      "--------------------------------------------------\n",
      "Causal mask (0 = allowed, 1 = masked):\n",
      "[[0. 1. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "Masked attention weights:\n",
      "[[1.         0.         0.         0.        ]\n",
      " [0.22872764 0.77127236 0.         0.        ]\n",
      " [0.262192   0.52686117 0.21094683 0.        ]\n",
      " [0.19937022 0.68523473 0.05628979 0.05910526]]\n",
      "Note: Future positions have ~0 attention weight\n",
      "\n",
      "4. MULTI-HEAD ATTENTION\n",
      "--------------------------------------------------\n",
      "Number of heads: 4\n",
      "Dimension per head: 16\n",
      "Total dimension: 64\n",
      "Multi-head attention output shape: (4, 64)\n",
      "\n",
      "5. FEED-FORWARD NETWORK\n",
      "--------------------------------------------------\n",
      "FFN input shape: (1, 64)\n",
      "FFN output shape: (1, 64)\n",
      "Hidden layer dimension: 256\n",
      "\n",
      "6. COMPLETE ENCODER LAYER\n",
      "--------------------------------------------------\n",
      "Encoder layer output shape: (4, 64)\n",
      "\n",
      "=== KEY MATHEMATICAL CONCEPTS ===\n",
      "--------------------------------------------------\n",
      "1. Attention allows the model to focus on different parts of the input\n",
      "2. Scaling by √d_k prevents gradient vanishing in softmax\n",
      "3. Multi-head attention learns different types of relationships\n",
      "4. Residual connections help with gradient flow\n",
      "5. Layer normalization stabilizes training\n",
      "6. FFN adds non-linearity and increases model capacity\n",
      "\n",
      "\n",
      "=== MATHEMATICAL SUMMARY ===\n",
      "--------------------------------------------------\n",
      "Core Transformer Equations:\n",
      "\n",
      "1. Scaled Dot-Product Attention:\n",
      "   Attention(Q,K,V) = softmax(QK^T/√d_k)V\n",
      "\n",
      "2. Multi-Head Attention:\n",
      "   MultiHead(Q,K,V) = Concat(head_1,...,head_h)W^O\n",
      "   where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\n",
      "\n",
      "3. Position-wise FFN:\n",
      "   FFN(x) = max(0, xW_1 + b_1)W_2 + b_2\n",
      "\n",
      "4. Positional Encoding:\n",
      "   PE(pos,2i) = sin(pos/10000^(2i/d_model))\n",
      "   PE(pos,2i+1) = cos(pos/10000^(2i/d_model))\n",
      "\n",
      "5. Layer Normalization:\n",
      "   LayerNorm(x) = γ * (x-μ)/σ + β\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class TransformerExplained:\n",
    "    \"\"\"\n",
    "    A simplified implementation of the Transformer architecture\n",
    "    with detailed mathematical explanations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model=512, n_heads=8, d_ff=2048, vocab_size=10000):\n",
    "        \"\"\"\n",
    "        Initialize transformer parameters.\n",
    "        \n",
    "        Args:\n",
    "            d_model: Dimension of the model (embedding size)\n",
    "            n_heads: Number of attention heads\n",
    "            d_ff: Dimension of feed-forward network\n",
    "            vocab_size: Size of vocabulary\n",
    "        \"\"\"\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads  # Dimension per head\n",
    "        self.d_ff = d_ff\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Initialize random weights (in practice, these would be learned)\n",
    "        self.embedding = np.random.randn(vocab_size, d_model) * 0.1\n",
    "        \n",
    "    def positional_encoding(self, seq_len, d_model):\n",
    "        \"\"\"\n",
    "        Create positional encodings using sine and cosine functions.\n",
    "        \n",
    "        Mathematical formula:\n",
    "        PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "        PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "        \n",
    "        Where:\n",
    "        - pos is the position in the sequence\n",
    "        - i is the dimension index\n",
    "        \"\"\"\n",
    "        PE = np.zeros((seq_len, d_model))\n",
    "        \n",
    "        for pos in range(seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                # Calculate the division term\n",
    "                div_term = 10000 ** (i / d_model)\n",
    "                \n",
    "                # Apply sine to even indices\n",
    "                PE[pos, i] = math.sin(pos / div_term)\n",
    "                \n",
    "                # Apply cosine to odd indices\n",
    "                if i + 1 < d_model:\n",
    "                    PE[pos, i + 1] = math.cos(pos / div_term)\n",
    "        \n",
    "        return PE\n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Compute scaled dot-product attention.\n",
    "        \n",
    "        Mathematical formula:\n",
    "        Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V\n",
    "        \n",
    "        Where:\n",
    "        - Q: Query matrix (seq_len, d_k)\n",
    "        - K: Key matrix (seq_len, d_k)\n",
    "        - V: Value matrix (seq_len, d_k)\n",
    "        - d_k: Dimension of each head\n",
    "        \n",
    "        Steps:\n",
    "        1. Compute QK^T (matrix multiplication)\n",
    "        2. Scale by 1/sqrt(d_k) to prevent gradient vanishing\n",
    "        3. Apply mask (optional, for decoder self-attention)\n",
    "        4. Apply softmax to get attention weights\n",
    "        5. Multiply by V to get weighted values\n",
    "        \"\"\"\n",
    "        # Step 1: Compute QK^T\n",
    "        # Shape: (seq_len, seq_len)\n",
    "        scores = np.dot(Q, K.T)\n",
    "        \n",
    "        # Step 2: Scale by sqrt(d_k)\n",
    "        # This prevents the dot products from growing too large\n",
    "        scores = scores / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Step 3: Apply mask if provided (e.g., for causal attention)\n",
    "        if mask is not None:\n",
    "            scores = scores + mask * -1e9\n",
    "        \n",
    "        # Step 4: Apply softmax to get attention weights\n",
    "        # Each row sums to 1\n",
    "        attention_weights = self.softmax(scores)\n",
    "        \n",
    "        # Step 5: Apply attention weights to values\n",
    "        # Shape: (seq_len, d_k)\n",
    "        output = np.dot(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def multi_head_attention(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Implement multi-head attention mechanism.\n",
    "        \n",
    "        Mathematical process:\n",
    "        1. Project Q, K, V into h different subspaces using linear transformations\n",
    "        2. Apply scaled dot-product attention in parallel for each head\n",
    "        3. Concatenate the outputs\n",
    "        4. Apply final linear transformation\n",
    "        \n",
    "        Formula:\n",
    "        MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O\n",
    "        where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\n",
    "        \"\"\"\n",
    "        seq_len = query.shape[0]\n",
    "        \n",
    "        # Initialize weight matrices (normally learned)\n",
    "        W_q = np.random.randn(self.d_model, self.d_model) * 0.1\n",
    "        W_k = np.random.randn(self.d_model, self.d_model) * 0.1\n",
    "        W_v = np.random.randn(self.d_model, self.d_model) * 0.1\n",
    "        W_o = np.random.randn(self.d_model, self.d_model) * 0.1\n",
    "        \n",
    "        # Project to get Q, K, V for all heads at once\n",
    "        Q = np.dot(query, W_q)  # (seq_len, d_model)\n",
    "        K = np.dot(key, W_k)    # (seq_len, d_model)\n",
    "        V = np.dot(value, W_v)  # (seq_len, d_model)\n",
    "        \n",
    "        # Reshape to separate heads\n",
    "        Q = Q.reshape(seq_len, self.n_heads, self.d_k)\n",
    "        K = K.reshape(seq_len, self.n_heads, self.d_k)\n",
    "        V = V.reshape(seq_len, self.n_heads, self.d_k)\n",
    "        \n",
    "        # Apply attention for each head\n",
    "        all_heads = []\n",
    "        attention_weights_all = []\n",
    "        \n",
    "        for i in range(self.n_heads):\n",
    "            # Extract each head\n",
    "            Q_head = Q[:, i, :]  # (seq_len, d_k)\n",
    "            K_head = K[:, i, :]  # (seq_len, d_k)\n",
    "            V_head = V[:, i, :]  # (seq_len, d_k)\n",
    "            \n",
    "            # Apply scaled dot-product attention\n",
    "            head_output, attn_weights = self.scaled_dot_product_attention(\n",
    "                Q_head, K_head, V_head, mask\n",
    "            )\n",
    "            all_heads.append(head_output)\n",
    "            attention_weights_all.append(attn_weights)\n",
    "        \n",
    "        # Concatenate all heads\n",
    "        # Shape: (seq_len, d_model)\n",
    "        concat_output = np.concatenate(all_heads, axis=1)\n",
    "        \n",
    "        # Final linear transformation\n",
    "        output = np.dot(concat_output, W_o)\n",
    "        \n",
    "        return output, attention_weights_all\n",
    "    \n",
    "    def feed_forward_network(self, x):\n",
    "        \"\"\"\n",
    "        Implement position-wise feed-forward network.\n",
    "        \n",
    "        Mathematical formula:\n",
    "        FFN(x) = max(0, xW_1 + b_1)W_2 + b_2\n",
    "        \n",
    "        This is a simple 2-layer neural network with ReLU activation.\n",
    "        Applied to each position separately and identically.\n",
    "        \"\"\"\n",
    "        # Initialize weights and biases (normally learned)\n",
    "        W1 = np.random.randn(self.d_model, self.d_ff) * 0.1\n",
    "        b1 = np.zeros(self.d_ff)\n",
    "        W2 = np.random.randn(self.d_ff, self.d_model) * 0.1\n",
    "        b2 = np.zeros(self.d_model)\n",
    "        \n",
    "        # First linear transformation\n",
    "        hidden = np.dot(x, W1) + b1\n",
    "        \n",
    "        # ReLU activation\n",
    "        hidden = np.maximum(0, hidden)\n",
    "        \n",
    "        # Second linear transformation\n",
    "        output = np.dot(hidden, W2) + b2\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def layer_norm(self, x, epsilon=1e-6):\n",
    "        \"\"\"\n",
    "        Apply layer normalization.\n",
    "        \n",
    "        Mathematical formula:\n",
    "        LayerNorm(x) = γ * (x - μ) / σ + β\n",
    "        \n",
    "        Where:\n",
    "        - μ: mean of x\n",
    "        - σ: standard deviation of x\n",
    "        - γ: learned scale parameter (set to 1 here)\n",
    "        - β: learned shift parameter (set to 0 here)\n",
    "        \"\"\"\n",
    "        # Calculate mean and variance\n",
    "        mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        variance = np.var(x, axis=-1, keepdims=True)\n",
    "        \n",
    "        # Normalize\n",
    "        x_norm = (x - mean) / np.sqrt(variance + epsilon)\n",
    "        \n",
    "        # In practice, γ and β would be learned parameters\n",
    "        gamma = np.ones(x.shape[-1])\n",
    "        beta = np.zeros(x.shape[-1])\n",
    "        \n",
    "        return gamma * x_norm + beta\n",
    "    \n",
    "    def encoder_layer(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Implement a single encoder layer.\n",
    "        \n",
    "        Components:\n",
    "        1. Multi-head self-attention\n",
    "        2. Add & Norm\n",
    "        3. Feed-forward network\n",
    "        4. Add & Norm\n",
    "        \"\"\"\n",
    "        # Self-attention\n",
    "        attn_output, _ = self.multi_head_attention(x, x, x, mask)\n",
    "        \n",
    "        # Add & Norm (residual connection + layer normalization)\n",
    "        x = self.layer_norm(x + attn_output)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        ff_output = self.feed_forward_network(x)\n",
    "        \n",
    "        # Add & Norm\n",
    "        x = self.layer_norm(x + ff_output)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def decoder_layer(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Implement a single decoder layer.\n",
    "        \n",
    "        Components:\n",
    "        1. Masked multi-head self-attention\n",
    "        2. Add & Norm\n",
    "        3. Multi-head cross-attention (attending to encoder output)\n",
    "        4. Add & Norm\n",
    "        5. Feed-forward network\n",
    "        6. Add & Norm\n",
    "        \"\"\"\n",
    "        # Masked self-attention\n",
    "        self_attn_output, _ = self.multi_head_attention(x, x, x, tgt_mask)\n",
    "        x = self.layer_norm(x + self_attn_output)\n",
    "        \n",
    "        # Cross-attention to encoder output\n",
    "        cross_attn_output, _ = self.multi_head_attention(\n",
    "            x, encoder_output, encoder_output, src_mask\n",
    "        )\n",
    "        x = self.layer_norm(x + cross_attn_output)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        ff_output = self.feed_forward_network(x)\n",
    "        x = self.layer_norm(x + ff_output)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def create_causal_mask(self, seq_len):\n",
    "        \"\"\"\n",
    "        Create a causal mask for decoder self-attention.\n",
    "        This prevents the decoder from attending to future positions.\n",
    "        \n",
    "        Returns upper triangular matrix with -inf values.\n",
    "        \"\"\"\n",
    "        mask = np.triu(np.ones((seq_len, seq_len)), k=1)\n",
    "        return mask\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \"\"\"Compute softmax values for each row of x.\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "    \n",
    "    def demonstrate_attention(self):\n",
    "        \"\"\"\n",
    "        Demonstrate attention mechanism with a simple example.\n",
    "        \"\"\"\n",
    "        print(\"=== TRANSFORMER ARCHITECTURE DEMONSTRATION ===\\n\")\n",
    "        \n",
    "        # Example: Simple sequence of 4 tokens\n",
    "        seq_len = 4\n",
    "        \n",
    "        print(\"1. INPUT EMBEDDING AND POSITIONAL ENCODING\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Simulate token IDs (must be within vocab_size range)\n",
    "        token_ids = np.array([101, 253, 37, 102])  # Example token IDs\n",
    "        print(f\"Token IDs: {token_ids}\")\n",
    "        \n",
    "        # Get embeddings\n",
    "        embeddings = self.embedding[token_ids]  # (seq_len, d_model)\n",
    "        print(f\"Embedding shape: {embeddings.shape}\")\n",
    "        \n",
    "        # Add positional encoding\n",
    "        pos_encoding = self.positional_encoding(seq_len, self.d_model)\n",
    "        input_embeddings = embeddings + pos_encoding\n",
    "        print(f\"After adding positional encoding: {input_embeddings.shape}\")\n",
    "        \n",
    "        print(\"\\n2. SELF-ATTENTION MECHANISM\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Simplified attention with small dimensions for visualization\n",
    "        d_k_demo = 4\n",
    "        Q = np.random.randn(seq_len, d_k_demo)\n",
    "        K = np.random.randn(seq_len, d_k_demo)\n",
    "        V = np.random.randn(seq_len, d_k_demo)\n",
    "        \n",
    "        print(f\"Query (Q) shape: {Q.shape}\")\n",
    "        print(f\"Key (K) shape: {K.shape}\")\n",
    "        print(f\"Value (V) shape: {V.shape}\")\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = np.dot(Q, K.T) / math.sqrt(d_k_demo)\n",
    "        print(f\"\\nAttention scores (QK^T/√d_k):\")\n",
    "        print(scores)\n",
    "        \n",
    "        # Apply softmax\n",
    "        attention_weights = self.softmax(scores)\n",
    "        print(f\"\\nAttention weights (after softmax):\")\n",
    "        print(attention_weights)\n",
    "        print(f\"Row sums (should be 1): {attention_weights.sum(axis=1)}\")\n",
    "        \n",
    "        # Apply attention to values\n",
    "        output = np.dot(attention_weights, V)\n",
    "        print(f\"\\nAttention output shape: {output.shape}\")\n",
    "        \n",
    "        print(\"\\n3. CAUSAL MASKING (for Decoder)\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Create causal mask\n",
    "        causal_mask = self.create_causal_mask(seq_len)\n",
    "        print(\"Causal mask (0 = allowed, 1 = masked):\")\n",
    "        print(causal_mask)\n",
    "        \n",
    "        # Apply mask to scores\n",
    "        masked_scores = scores + causal_mask * -1e9\n",
    "        masked_attention = self.softmax(masked_scores)\n",
    "        print(\"\\nMasked attention weights:\")\n",
    "        print(masked_attention)\n",
    "        print(\"Note: Future positions have ~0 attention weight\")\n",
    "        \n",
    "        print(\"\\n4. MULTI-HEAD ATTENTION\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Number of heads: {self.n_heads}\")\n",
    "        print(f\"Dimension per head: {self.d_k}\")\n",
    "        print(f\"Total dimension: {self.d_model}\")\n",
    "        \n",
    "        # Run multi-head attention\n",
    "        mha_output, attn_weights = self.multi_head_attention(\n",
    "            input_embeddings[:seq_len], \n",
    "            input_embeddings[:seq_len], \n",
    "            input_embeddings[:seq_len]\n",
    "        )\n",
    "        print(f\"Multi-head attention output shape: {mha_output.shape}\")\n",
    "        \n",
    "        print(\"\\n5. FEED-FORWARD NETWORK\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Example FFN computation\n",
    "        x_example = np.random.randn(1, self.d_model)\n",
    "        ff_output = self.feed_forward_network(x_example)\n",
    "        print(f\"FFN input shape: {x_example.shape}\")\n",
    "        print(f\"FFN output shape: {ff_output.shape}\")\n",
    "        print(f\"Hidden layer dimension: {self.d_ff}\")\n",
    "        \n",
    "        print(\"\\n6. COMPLETE ENCODER LAYER\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        encoder_output = self.encoder_layer(input_embeddings[:seq_len])\n",
    "        print(f\"Encoder layer output shape: {encoder_output.shape}\")\n",
    "        \n",
    "        print(\"\\n=== KEY MATHEMATICAL CONCEPTS ===\")\n",
    "        print(\"-\" * 50)\n",
    "        print(\"1. Attention allows the model to focus on different parts of the input\")\n",
    "        print(\"2. Scaling by √d_k prevents gradient vanishing in softmax\")\n",
    "        print(\"3. Multi-head attention learns different types of relationships\")\n",
    "        print(\"4. Residual connections help with gradient flow\")\n",
    "        print(\"5. Layer normalization stabilizes training\")\n",
    "        print(\"6. FFN adds non-linearity and increases model capacity\")\n",
    "\n",
    "\n",
    "# Run the demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    # Create transformer with smaller dimensions for clarity\n",
    "    transformer = TransformerExplained(\n",
    "        d_model=64,    # Smaller for demonstration\n",
    "        n_heads=4,\n",
    "        d_ff=256,\n",
    "        vocab_size=1000\n",
    "    )\n",
    "    \n",
    "    # Run the demonstration\n",
    "    transformer.demonstrate_attention()\n",
    "    \n",
    "    print(\"\\n\\n=== MATHEMATICAL SUMMARY ===\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Core Transformer Equations:\")\n",
    "    print()\n",
    "    print(\"1. Scaled Dot-Product Attention:\")\n",
    "    print(\"   Attention(Q,K,V) = softmax(QK^T/√d_k)V\")\n",
    "    print()\n",
    "    print(\"2. Multi-Head Attention:\")\n",
    "    print(\"   MultiHead(Q,K,V) = Concat(head_1,...,head_h)W^O\")\n",
    "    print(\"   where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\")\n",
    "    print()\n",
    "    print(\"3. Position-wise FFN:\")\n",
    "    print(\"   FFN(x) = max(0, xW_1 + b_1)W_2 + b_2\")\n",
    "    print()\n",
    "    print(\"4. Positional Encoding:\")\n",
    "    print(\"   PE(pos,2i) = sin(pos/10000^(2i/d_model))\")\n",
    "    print(\"   PE(pos,2i+1) = cos(pos/10000^(2i/d_model))\")\n",
    "    print()\n",
    "    print(\"5. Layer Normalization:\")\n",
    "    print(\"   LayerNorm(x) = γ * (x-μ)/σ + β\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
