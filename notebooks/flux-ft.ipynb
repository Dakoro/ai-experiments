{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ef0470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "Dataset.cleanup_cache_files\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"derekl35/alphonse-mucha-style\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc4a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub.utils import insecure_hashlib\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import T5EncoderModel\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "\n",
    "\n",
    "MAX_SEQ_LENGTH = 77\n",
    "OUTPUT_PATH = \"embeddings_alphonse_mucha.parquet\"\n",
    "DATASET_NAME = \"derekl35/alphonse-mucha-style\"\n",
    "MODEL_ID = \"black-forest-labs/FLUX.1-dev\"\n",
    "\n",
    "def generate_image_hash(image):\n",
    "    return insecure_hashlib.sha256(image.tobytes()).hexdigest()\n",
    "\n",
    "\n",
    "def load_flux_dev_pipeline():\n",
    "    id = \"black-forest-labs/FLUX.1-dev\"\n",
    "    text_encoder = T5EncoderModel.from_pretrained(id, subfolder=\"text_encoder_2\",\n",
    "                                                  load_in_4bit=True,\n",
    "                                                  bnb_4bit_quant_type=\"nf4\",\n",
    "                                                  llm_int8_enable_fp32_cpu_offload=True,\n",
    "                                                  bnb_4bit_compute_dtype=torch.float16)\n",
    "    pipeline = DiffusionPipeline.from_pretrained(\n",
    "        id, text_encoder_2=text_encoder, transformer=None, vae=None, device_map=\"balanced\"\n",
    "    )\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_embeddings(pipeline, prompts, max_sequence_length):\n",
    "    all_prompt_embeds = []\n",
    "    all_pooled_prompt_embeds = []\n",
    "    all_text_ids = []\n",
    "    for prompt in tqdm(prompts, desc=\"Encoding prompts.\"):\n",
    "        (\n",
    "            prompt_embeds,\n",
    "            pooled_prompt_embeds,\n",
    "            text_ids,\n",
    "        ) = pipeline.encode_prompt(prompt=prompt, prompt_2=None, max_sequence_length=max_sequence_length)\n",
    "        all_prompt_embeds.append(prompt_embeds)\n",
    "        all_pooled_prompt_embeds.append(pooled_prompt_embeds)\n",
    "        all_text_ids.append(text_ids)\n",
    "\n",
    "    max_memory = torch.cuda.max_memory_allocated() / 1024 / 1024 / 1024\n",
    "    print(f\"Max memory allocated: {max_memory:.3f} GB\")\n",
    "    return all_prompt_embeds, all_pooled_prompt_embeds, all_text_ids\n",
    "\n",
    "\n",
    "def get_embeddings(output_path):\n",
    "    dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "    image_prompts = {generate_image_hash(sample[\"image\"]): sample[\"text\"] for sample in dataset}\n",
    "    all_prompts = list(image_prompts.values())\n",
    "    print(f\"{len(all_prompts)=}\")\n",
    "\n",
    "    pipeline = load_flux_dev_pipeline()\n",
    "    all_prompt_embeds, all_pooled_prompt_embeds, all_text_ids = compute_embeddings(\n",
    "        pipeline, all_prompts, MAX_SEQ_LENGTH\n",
    "    )\n",
    "\n",
    "    data = []\n",
    "    for i, (image_hash, _) in enumerate(image_prompts.items()):\n",
    "        data.append((image_hash, all_prompt_embeds[i], all_pooled_prompt_embeds[i], all_text_ids[i]))\n",
    "    print(f\"{len(data)=}\")\n",
    "\n",
    "    # Create a DataFrame\n",
    "    embedding_cols = [\"prompt_embeds\", \"pooled_prompt_embeds\", \"text_ids\"]\n",
    "    df = pd.DataFrame(data, columns=[\"image_hash\"] + embedding_cols)\n",
    "    print(f\"{len(df)=}\")\n",
    "\n",
    "    # Convert embedding lists to arrays (for proper storage in parquet)\n",
    "    for col in embedding_cols:\n",
    "        df[col] = df[col].apply(lambda x: x.cpu().numpy().flatten().tolist())\n",
    "\n",
    "    # Save the dataframe to a parquet file\n",
    "    df.to_parquet(output_path)\n",
    "    print(f\"Data successfully serialized to {output_path}\")\n",
    "\n",
    "    del pipeline\n",
    "    del dataset\n",
    "    del image_prompts\n",
    "    del all_prompts\n",
    "    del all_prompt_embeds\n",
    "    del all_pooled_prompt_embeds\n",
    "    del all_text_ids\n",
    "    del df\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eee196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_embeddings('../data/flux-embed-img.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acac633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from accelerate import Accelerator, DistributedType\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import DistributedDataParallelKwargs, ProjectConfiguration, set_seed\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub.utils import insecure_hashlib\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, set_peft_model_state_dict\n",
    "from peft.utils import get_peft_model_state_dict\n",
    "from PIL.ImageOps import exif_transpose\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import crop\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import diffusers\n",
    "from diffusers import (\n",
    "    AutoencoderKL, BitsAndBytesConfig, FlowMatchEulerDiscreteScheduler,\n",
    "    FluxPipeline, FluxTransformer2DModel,\n",
    ")\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.training_utils import (\n",
    "    cast_training_params, compute_density_for_timestep_sampling,\n",
    "    compute_loss_weighting_for_sd3, free_memory,\n",
    ")\n",
    "from diffusers.utils import convert_unet_state_dict_to_peft, is_wandb_available\n",
    "from diffusers.utils.torch_utils import is_compiled_module\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dc9887",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DreamBoothDataset(Dataset):\n",
    "    def __init__(self, data_df_path, dataset_name, width, height, max_sequence_length=77):\n",
    "        self.width, self.height, self.max_sequence_length = width, height, max_sequence_length\n",
    "        self.data_df_path = Path(data_df_path)\n",
    "        if not self.data_df_path.exists():\n",
    "            raise ValueError(\"`data_df_path` doesn't exists.\")\n",
    "\n",
    "        dataset = load_dataset(dataset_name, split=\"train\")\n",
    "        self.instance_images = [sample[\"image\"] for sample in dataset]\n",
    "        self.image_hashes = [insecure_hashlib.sha256(img.tobytes()).hexdigest() for img in self.instance_images]\n",
    "        self.pixel_values = self._apply_transforms()\n",
    "        self.data_dict = self._map_embeddings()\n",
    "        self._length = len(self.instance_images)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        idx = index % len(self.instance_images)\n",
    "        hash_key = self.image_hashes[idx]\n",
    "        prompt_embeds, pooled_prompt_embeds, text_ids = self.data_dict[hash_key]\n",
    "        return {\n",
    "            \"instance_images\": self.pixel_values[idx],\n",
    "            \"prompt_embeds\": prompt_embeds,\n",
    "            \"pooled_prompt_embeds\": pooled_prompt_embeds,\n",
    "            \"text_ids\": text_ids,\n",
    "        }\n",
    "\n",
    "    def _apply_transforms(self):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((self.height, self.width), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.RandomCrop((self.height, self.width)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ])\n",
    "\n",
    "        pixel_values = []\n",
    "        for image in self.instance_images:\n",
    "            image = exif_transpose(image).convert(\"RGB\") if image.mode != \"RGB\" else exif_transpose(image)\n",
    "            pixel_values.append(transform(image))\n",
    "        return pixel_values\n",
    "\n",
    "    def _map_embeddings(self):\n",
    "        df = pd.read_parquet(self.data_df_path)\n",
    "        data_dict = {}\n",
    "        for _, row in df.iterrows():\n",
    "            prompt_embeds = torch.from_numpy(np.array(row[\"prompt_embeds\"]).reshape(self.max_sequence_length, 4096))\n",
    "            pooled_prompt_embeds = torch.from_numpy(np.array(row[\"pooled_prompt_embeds\"]).reshape(768))\n",
    "            text_ids = torch.from_numpy(np.array(row[\"text_ids\"]).reshape(77, 3))\n",
    "            data_dict[row[\"image_hash\"]] = (prompt_embeds, pooled_prompt_embeds, text_ids)\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d03c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([ex[\"instance_images\"] for ex in examples]).float()\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "    prompt_embeds = torch.stack([ex[\"prompt_embeds\"] for ex in examples])\n",
    "    pooled_prompt_embeds = torch.stack([ex[\"pooled_prompt_embeds\"] for ex in examples])\n",
    "    text_ids = torch.stack([ex[\"text_ids\"] for ex in examples])[0]\n",
    "\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"prompt_embeds\": prompt_embeds,\n",
    "        \"pooled_prompt_embeds\": pooled_prompt_embeds,\n",
    "        \"text_ids\": text_ids,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edc8ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetuned(args):\n",
    "    # Setup accelerator\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        mixed_precision=args.mixed_precision,\n",
    "        log_with=args.report_to,\n",
    "        project_config=ProjectConfiguration(project_dir=args.output_dir, logging_dir=Path(args.output_dir, \"logs\")),\n",
    "        kwargs_handlers=[DistributedDataParallelKwargs(find_unused_parameters=True)],\n",
    "    )\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\", level=logging.INFO)\n",
    "    if accelerator.is_local_main_process:\n",
    "        transformers.utils.logging.set_verbosity_warning()\n",
    "        diffusers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "        diffusers.utils.logging.set_verbosity_error()\n",
    "\n",
    "    set_seed(args.seed) if args.seed is not None else None\n",
    "    os.makedirs(args.output_dir, exist_ok=True) if accelerator.is_main_process else None\n",
    "\n",
    "    # Load models with quantization\n",
    "    noise_scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "    noise_scheduler_copy = copy.deepcopy(noise_scheduler)\n",
    "\n",
    "    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\")\n",
    "\n",
    "    nf4_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16)\n",
    "    transformer = FluxTransformer2DModel.from_pretrained(\n",
    "        args.pretrained_model_name_or_path, subfolder=\"transformer\",\n",
    "        quantization_config=nf4_config, torch_dtype=torch.float16\n",
    "    )\n",
    "    transformer = prepare_model_for_kbit_training(transformer, use_gradient_checkpointing=False)\n",
    "\n",
    "    # Freeze models and setup LoRA\n",
    "    transformer.requires_grad_(False)\n",
    "    vae.requires_grad_(False)\n",
    "    vae.to(accelerator.device, dtype=torch.float16)\n",
    "    if args.gradient_checkpointing:\n",
    "        transformer.enable_gradient_checkpointing()\n",
    "\n",
    "    # now we will add new LoRA weights to the attention layers\n",
    "    transformer_lora_config = LoraConfig(\n",
    "        r=args.rank,\n",
    "        lora_alpha=args.rank,\n",
    "        init_lora_weights=\"gaussian\",\n",
    "        target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
    "    )\n",
    "    transformer.add_adapter(transformer_lora_config)\n",
    "\n",
    "    print(f\"trainable params: {transformer.num_parameters(only_trainable=True)} || all params: {transformer.num_parameters()}\")\n",
    "\n",
    "    # Setup optimizer\n",
    "    import bitsandbytes as bnb\n",
    "    optimizer = bnb.optim.AdamW8bit(\n",
    "        [{\"params\": list(filter(lambda p: p.requires_grad, transformer.parameters())), \"lr\": args.learning_rate}],\n",
    "        betas=(0.9, 0.999), weight_decay=1e-04, eps=1e-08\n",
    "    )\n",
    "\n",
    "    # Setup dataset and dataloader\n",
    "    train_dataset = DreamBoothDataset(args.data_df_path, \"derekl35/alphonse-mucha-style\", args.width, args.height)\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=args.train_batch_size, shuffle=True, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # Cache latents\n",
    "    vae_config = vae.config\n",
    "    latents_cache = []\n",
    "    for batch in tqdm(train_dataloader, desc=\"Caching latents\"):\n",
    "        with torch.no_grad():\n",
    "            pixel_values = batch[\"pixel_values\"].to(accelerator.device, dtype=torch.float16)\n",
    "            latents_cache.append(vae.encode(pixel_values).latent_dist)\n",
    "\n",
    "    del vae\n",
    "    free_memory()\n",
    "\n",
    "    # Setup scheduler and training steps\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "    args.max_train_steps = args.max_train_steps or args.num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "    lr_scheduler = get_scheduler(\"constant\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=args.max_train_steps)\n",
    "\n",
    "    # Prepare for training\n",
    "    transformer, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(transformer, optimizer, train_dataloader, lr_scheduler)\n",
    "\n",
    "    # Register save/load hooks\n",
    "    def unwrap_model(model):\n",
    "        model = accelerator.unwrap_model(model)\n",
    "        return model._orig_mod if is_compiled_module(model) else model\n",
    "\n",
    "    def save_model_hook(models, weights, output_dir):\n",
    "        if accelerator.is_main_process:\n",
    "            for model in models:\n",
    "                if isinstance(unwrap_model(model), type(unwrap_model(transformer))):\n",
    "                    lora_layers = get_peft_model_state_dict(unwrap_model(model))\n",
    "                    FluxPipeline.save_lora_weights(output_dir, transformer_lora_layers=lora_layers, text_encoder_lora_layers=None)\n",
    "                    weights.pop() if weights else None\n",
    "\n",
    "    accelerator.register_save_state_pre_hook(save_model_hook)\n",
    "    cast_training_params([transformer], dtype=torch.float32) if args.mixed_precision == \"fp16\" else None\n",
    "\n",
    "    # Initialize tracking\n",
    "    accelerator.init_trackers(\"dreambooth-flux-dev-lora-alphonse-mucha\", config=vars(args)) if accelerator.is_main_process else None\n",
    "\n",
    "    # Training loop\n",
    "    def get_sigmas(timesteps, n_dim=4, dtype=torch.float32):\n",
    "        sigmas = noise_scheduler_copy.sigmas.to(device=accelerator.device, dtype=dtype)\n",
    "        schedule_timesteps = noise_scheduler_copy.timesteps.to(accelerator.device)\n",
    "        step_indices = [(schedule_timesteps == t).nonzero().item() for t in timesteps.to(accelerator.device)]\n",
    "        sigma = sigmas[step_indices].flatten()\n",
    "        while len(sigma.shape) < n_dim:\n",
    "            sigma = sigma.unsqueeze(-1)\n",
    "        return sigma\n",
    "\n",
    "    global_step = 0\n",
    "    progress_bar = tqdm(range(args.max_train_steps), desc=\"Steps\", disable=not accelerator.is_local_main_process)\n",
    "\n",
    "    for epoch in range(args.num_train_epochs):\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            with accelerator.accumulate([transformer]):\n",
    "                # Get cached latents\n",
    "                model_input = latents_cache[step].sample()\n",
    "                model_input = (model_input - vae_config.shift_factor) * vae_config.scaling_factor\n",
    "                model_input = model_input.to(dtype=torch.float16)\n",
    "\n",
    "                # Prepare inputs\n",
    "                latent_image_ids = FluxPipeline._prepare_latent_image_ids(\n",
    "                    model_input.shape[0], model_input.shape[2] // 2, model_input.shape[3] // 2,\n",
    "                    accelerator.device, torch.float16\n",
    "                )\n",
    "\n",
    "                noise = torch.randn_like(model_input)\n",
    "                bsz = model_input.shape[0]\n",
    "\n",
    "                u = compute_density_for_timestep_sampling(\"none\", bsz, 0.0, 1.0, 1.29)\n",
    "                indices = (u * noise_scheduler_copy.config.num_train_timesteps).long()\n",
    "                timesteps = noise_scheduler_copy.timesteps[indices].to(device=model_input.device)\n",
    "\n",
    "                sigmas = get_sigmas(timesteps, n_dim=model_input.ndim, dtype=model_input.dtype)\n",
    "                noisy_model_input = (1.0 - sigmas) * model_input + sigmas * noise\n",
    "\n",
    "                packed_noisy_model_input = FluxPipeline._pack_latents(\n",
    "                    noisy_model_input, model_input.shape[0], model_input.shape[1],\n",
    "                    model_input.shape[2], model_input.shape[3]\n",
    "                )\n",
    "\n",
    "                # Forward pass\n",
    "                guidance = torch.tensor([args.guidance_scale], device=accelerator.device).expand(bsz) if unwrap_model(transformer).config.guidance_embeds else None\n",
    "\n",
    "                model_pred = transformer(\n",
    "                    hidden_states=packed_noisy_model_input,\n",
    "                    timestep=timesteps / 1000,\n",
    "                    guidance=guidance,\n",
    "                    pooled_projections=batch[\"pooled_prompt_embeds\"].to(accelerator.device, dtype=torch.float16),\n",
    "                    encoder_hidden_states=batch[\"prompt_embeds\"].to(accelerator.device, dtype=torch.float16),\n",
    "                    txt_ids=batch[\"text_ids\"].to(accelerator.device, dtype=torch.float16),\n",
    "                    img_ids=latent_image_ids,\n",
    "                    return_dict=False,\n",
    "                )[0]\n",
    "\n",
    "                vae_scale_factor = 2 ** (len(vae_config.block_out_channels) - 1)\n",
    "                model_pred = FluxPipeline._unpack_latents(\n",
    "                    model_pred, model_input.shape[2] * vae_scale_factor,\n",
    "                    model_input.shape[3] * vae_scale_factor, vae_scale_factor\n",
    "                )\n",
    "\n",
    "                # Compute loss\n",
    "                weighting = compute_loss_weighting_for_sd3(\"none\", sigmas)\n",
    "                target = noise - model_input\n",
    "                loss = torch.mean((weighting.float() * (model_pred.float() - target.float()) ** 2).reshape(target.shape[0], -1), 1).mean()\n",
    "\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(transformer.parameters(), 1.0)\n",
    "\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                global_step += 1\n",
    "\n",
    "                # Checkpointing\n",
    "                if global_step % args.checkpointing_steps == 0 and (accelerator.is_main_process or accelerator.distributed_type == DistributedType.DEEPSPEED):\n",
    "                    save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n",
    "                    accelerator.save_state(save_path)\n",
    "\n",
    "            # Logging\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "\n",
    "            if global_step >= args.max_train_steps:\n",
    "                break\n",
    "\n",
    "    # Final save\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        transformer_lora_layers = get_peft_model_state_dict(unwrap_model(transformer))\n",
    "        FluxPipeline.save_lora_weights(args.output_dir, transformer_lora_layers=transformer_lora_layers, text_encoder_lora_layers=None)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Pipeline memory usage: {torch.cuda.max_memory_reserved() / 1024**3:.3f} GB\")\n",
    "    else:\n",
    "        print(\"Training completed. GPU not available for memory tracking.\")\n",
    "\n",
    "    accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b80e77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    pretrained_model_name_or_path = \"black-forest-labs/FLUX.1-dev\"\n",
    "    data_df_path = \"../data/flux_embed-img.parquet\"\n",
    "    output_dir = \"lora_flux_nf4\"\n",
    "    mixed_precision = \"fp16\"\n",
    "    weighting_scheme = \"none\"\n",
    "    width, height = 512, 768\n",
    "    train_batch_size = 1\n",
    "    learning_rate = 1e-4\n",
    "    guidance_scale = 1.0\n",
    "    report_to = \"wandb\"\n",
    "    gradient_accumulation_steps = 4\n",
    "    gradient_checkpointing = True\n",
    "    rank = 4\n",
    "    max_train_steps = 100\n",
    "    seed = 0\n",
    "    checkpointing_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689ac3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned(Args())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
