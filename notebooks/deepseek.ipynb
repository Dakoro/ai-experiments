{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e3fd271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Prompt: Solve x + 2 = 5.... | Response: <think> Simulated response </think> | Combined Reward: 1.0799 | Loss: -1.0799\n",
      "Prompt: What is the capital of France?... | Response: <think> Simulated response </think> | Combined Reward: 1.0788 | Loss: -1.0788\n",
      "Prompt: Explain Pythagoras' theorem.... | Response: <think> Simulated response </think> | Combined Reward: 1.0861 | Loss: -1.0861\n",
      "Epoch 2/10\n",
      "Prompt: Solve x + 2 = 5.... | Response: <think> Simulated response </think> | Combined Reward: 1.0929 | Loss: -1.0929\n",
      "Prompt: What is the capital of France?... | Response: <think> Simulated response </think> | Combined Reward: 1.0695 | Loss: -1.0695\n",
      "Prompt: Explain Pythagoras' theorem.... | Response: <think> Simulated response </think> | Combined Reward: 1.0800 | Loss: -1.0800\n",
      "Epoch 3/10\n",
      "Prompt: Solve x + 2 = 5.... | Response: <think> Simulated response </think> | Combined Reward: 1.0747 | Loss: -1.0747\n",
      "Prompt: What is the capital of France?... | Response: <think> Simulated response </think> | Combined Reward: 1.0701 | Loss: -1.0701\n",
      "Prompt: Explain Pythagoras' theorem.... | Response: <think> Simulated response </think> | Combined Reward: 1.0831 | Loss: -1.0831\n",
      "Epoch 4/10\n",
      "Prompt: Solve x + 2 = 5.... | Response: <think> Simulated response </think> | Combined Reward: 1.0733 | Loss: -1.0733\n",
      "Prompt: What is the capital of France?... | Response: <think> Simulated response </think> | Combined Reward: 1.0925 | Loss: -1.0925\n",
      "Prompt: Explain Pythagoras' theorem.... | Response: <think> Simulated response </think> | Combined Reward: 1.0684 | Loss: -1.0684\n",
      "Epoch 5/10\n",
      "Prompt: Solve x + 2 = 5.... | Response: <think> Simulated response </think> | Combined Reward: 1.0713 | Loss: -1.0713\n",
      "Prompt: What is the capital of France?... | Response: <think> Simulated response </think> | Combined Reward: 1.0739 | Loss: -1.0739\n",
      "Prompt: Explain Pythagoras' theorem.... | Response: <think> Simulated response </think> | Combined Reward: 1.0737 | Loss: -1.0737\n",
      "Epoch 6/10\n",
      "Prompt: Solve x + 2 = 5.... | Response: <think> Simulated response </think> | Combined Reward: 1.0886 | Loss: -1.0886\n",
      "Prompt: What is the capital of France?... | Response: <think> Simulated response </think> | Combined Reward: 1.0783 | Loss: -1.0783\n",
      "Prompt: Explain Pythagoras' theorem.... | Response: <think> Simulated response </think> | Combined Reward: 1.0778 | Loss: -1.0778\n",
      "Epoch 7/10\n",
      "Prompt: Solve x + 2 = 5.... | Response: <think> Simulated response </think> | Combined Reward: 1.0852 | Loss: -1.0852\n",
      "Prompt: What is the capital of France?... | Response: <think> Simulated response </think> | Combined Reward: 1.0789 | Loss: -1.0789\n",
      "Prompt: Explain Pythagoras' theorem.... | Response: <think> Simulated response </think> | Combined Reward: 1.0780 | Loss: -1.0780\n",
      "Epoch 8/10\n",
      "Prompt: Solve x + 2 = 5.... | Response: <think> Simulated response </think> | Combined Reward: 1.0852 | Loss: -1.0852\n",
      "Prompt: What is the capital of France?... | Response: <think> Simulated response </think> | Combined Reward: 1.0752 | Loss: -1.0752\n",
      "Prompt: Explain Pythagoras' theorem.... | Response: <think> Simulated response </think> | Combined Reward: 1.0751 | Loss: -1.0751\n",
      "Epoch 9/10\n",
      "Prompt: Solve x + 2 = 5.... | Response: <think> Simulated response </think> | Combined Reward: 1.0965 | Loss: -1.0965\n",
      "Prompt: What is the capital of France?... | Response: <think> Simulated response </think> | Combined Reward: 1.0723 | Loss: -1.0723\n",
      "Prompt: Explain Pythagoras' theorem.... | Response: <think> Simulated response </think> | Combined Reward: 1.0736 | Loss: -1.0736\n",
      "Epoch 10/10\n",
      "Prompt: Solve x + 2 = 5.... | Response: <think> Simulated response </think> | Combined Reward: 1.0803 | Loss: -1.0803\n",
      "Prompt: What is the capital of France?... | Response: <think> Simulated response </think> | Combined Reward: 1.0863 | Loss: -1.0863\n",
      "Prompt: Explain Pythagoras' theorem.... | Response: <think> Simulated response </think> | Combined Reward: 1.0781 | Loss: -1.0781\n",
      "Training complete.\n",
      "Distillation Epoch 1/5\n",
      "Prompt: Solve x + 2 = 5.... | Distillation Loss: 0.0355\n",
      "Prompt: What is the capital of France?... | Distillation Loss: 0.0359\n",
      "Prompt: Explain Pythagoras' theorem.... | Distillation Loss: 0.0341\n",
      "Distillation Epoch 2/5\n",
      "Prompt: Solve x + 2 = 5.... | Distillation Loss: 0.0323\n",
      "Prompt: What is the capital of France?... | Distillation Loss: 0.0312\n",
      "Prompt: Explain Pythagoras' theorem.... | Distillation Loss: 0.0311\n",
      "Distillation Epoch 3/5\n",
      "Prompt: Solve x + 2 = 5.... | Distillation Loss: 0.0302\n",
      "Prompt: What is the capital of France?... | Distillation Loss: 0.0284\n",
      "Prompt: Explain Pythagoras' theorem.... | Distillation Loss: 0.0276\n",
      "Distillation Epoch 4/5\n",
      "Prompt: Solve x + 2 = 5.... | Distillation Loss: 0.0268\n",
      "Prompt: What is the capital of France?... | Distillation Loss: 0.0257\n",
      "Prompt: Explain Pythagoras' theorem.... | Distillation Loss: 0.0254\n",
      "Distillation Epoch 5/5\n",
      "Prompt: Solve x + 2 = 5.... | Distillation Loss: 0.0246\n",
      "Prompt: What is the capital of France?... | Distillation Loss: 0.0239\n",
      "Prompt: Explain Pythagoras' theorem.... | Distillation Loss: 0.0233\n",
      "Distillation complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from geoopt import PoincareBall\n",
    "from geoopt.optim import RiemannianAdam\n",
    "import random\n",
    "\n",
    "# =============================================================================\n",
    "#                            MODEL DEFINITION\n",
    "# =============================================================================\n",
    "class SmallRLModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SmallRLModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.manifold = PoincareBall(c=1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.manifold.expmap0(x)  # Map to hyperbolic space\n",
    "        x = self.fc2(self.manifold.logmap0(x))  # Back to Euclidean space\n",
    "        return x\n",
    "\n",
    "class SmallerRLModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SmallerRLModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# =============================================================================\n",
    "#                            REWARD FUNCTIONS\n",
    "# =============================================================================\n",
    "def compute_accuracy_reward(output, target):\n",
    "    \"\"\"Reward based on cosine similarity between output and target embeddings.\"\"\"\n",
    "    cos_sim = F.cosine_similarity(output, target, dim=-1)\n",
    "    return cos_sim.mean().item()\n",
    "\n",
    "def compute_format_reward(response, required_format):\n",
    "    \"\"\"Reward for adhering to the required format.\"\"\"\n",
    "    if required_format in response:\n",
    "        return 1.0\n",
    "    return -1.0\n",
    "\n",
    "def compute_combined_reward(output, target, response, required_format):\n",
    "    \"\"\"Combine accuracy and format rewards.\"\"\"\n",
    "    accuracy_reward = compute_accuracy_reward(output, target)\n",
    "    format_reward = compute_format_reward(response, required_format)\n",
    "    return accuracy_reward + format_reward\n",
    "\n",
    "# =============================================================================\n",
    "#                            TRAINING FUNCTION\n",
    "# =============================================================================\n",
    "def train_with_rl(\n",
    "    model,\n",
    "    optimizer,\n",
    "    dataset,\n",
    "    input_dim,\n",
    "    output_dim,\n",
    "    epochs=5,\n",
    "    max_seq_len=50\n",
    "):\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        for sample in dataset:\n",
    "            # Extract prompt and target embedding (dummy data here for simplicity)\n",
    "            prompt = sample[\"Prompt\"]\n",
    "            target_text = sample[\"Target\"]\n",
    "\n",
    "            # Convert text to embeddings (random here for demo purposes)\n",
    "            input_embedding = torch.rand((1, max_seq_len, input_dim)).to(device)\n",
    "            target_embedding = torch.rand((1, max_seq_len, output_dim)).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output_embedding = model(input_embedding)\n",
    "\n",
    "            # Decode output embedding to simulate model response\n",
    "            model_response = \"<think> Simulated response </think>\"  # Simulated decoding for demo purposes\n",
    "\n",
    "            # Compute rewards\n",
    "            required_format = \"<think>\"\n",
    "            combined_reward = compute_combined_reward(\n",
    "                output_embedding, target_embedding, model_response, required_format\n",
    "            )\n",
    "\n",
    "            # Loss = negative reward (maximize reward)\n",
    "            loss = -torch.tensor(combined_reward, requires_grad=True).to(device)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            history.append({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"prompt\": prompt,\n",
    "                \"reward\": combined_reward,\n",
    "                \"loss\": loss.item(),\n",
    "                \"response\": model_response\n",
    "            })\n",
    "\n",
    "            print(f\"Prompt: {prompt[:30]}... | Response: {model_response} | Combined Reward: {combined_reward:.4f} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "# =============================================================================\n",
    "#                      DISTILLATION FUNCTION\n",
    "# =============================================================================\n",
    "def distill_model(teacher_model, student_model, dataset, optimizer, epochs=5, max_seq_len=50):\n",
    "    \"\"\"Distill knowledge from the teacher model to the smaller student model.\"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Distillation Epoch {epoch + 1}/{epochs}\")\n",
    "        for sample in dataset:\n",
    "            # Extract prompt (dummy data here for simplicity)\n",
    "            prompt = sample[\"Prompt\"]\n",
    "\n",
    "            # Convert text to embeddings (random here for demo purposes)\n",
    "            input_embedding = torch.rand((1, max_seq_len, input_dim)).to(device)\n",
    "\n",
    "            # Teacher model output\n",
    "            with torch.no_grad():\n",
    "                teacher_output = teacher_model(input_embedding)\n",
    "\n",
    "            # Student model output\n",
    "            student_output = student_model(input_embedding)\n",
    "\n",
    "            # Loss = MSE between teacher and student outputs\n",
    "            loss = F.mse_loss(student_output, teacher_output)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(f\"Prompt: {prompt[:30]}... | Distillation Loss: {loss.item():.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "#                            MAIN EXECUTION\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # ---------------------------\n",
    "    # 1) Device Setup\n",
    "    # ---------------------------\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 2) Model and Optimizer Setup\n",
    "    # ---------------------------\n",
    "    input_dim = 128  # Example input dimension\n",
    "    hidden_dim = 256  # Hidden layer dimension\n",
    "    output_dim = 128  # Output dimension matches input for reconstruction\n",
    "\n",
    "    model = SmallRLModel(input_dim, hidden_dim, output_dim).to(device)\n",
    "    optimizer = RiemannianAdam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 3) Dataset (Dummy Data)\n",
    "    # ---------------------------\n",
    "    # Replace with actual reasoning task data\n",
    "    dataset = [\n",
    "        {\"Prompt\": \"Solve x + 2 = 5.\", \"Target\": \"x = 3.\"},\n",
    "        {\"Prompt\": \"What is the capital of France?\", \"Target\": \"Paris.\"},\n",
    "        {\"Prompt\": \"Explain Pythagoras' theorem.\", \"Target\": \"a^2 + b^2 = c^2 for a right triangle.\"}\n",
    "    ]\n",
    "\n",
    "    # ---------------------------\n",
    "    # 4) Training\n",
    "    # ---------------------------\n",
    "    history = train_with_rl(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        dataset=dataset,\n",
    "        input_dim=input_dim,\n",
    "        output_dim=output_dim,\n",
    "        epochs=10,\n",
    "        max_seq_len=50\n",
    "    )\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 5) Distillation\n",
    "    # ---------------------------\n",
    "    smaller_model = SmallerRLModel(input_dim, hidden_dim // 2, output_dim).to(device)\n",
    "    distill_optimizer = torch.optim.Adam(smaller_model.parameters(), lr=1e-4)\n",
    "\n",
    "    distill_model(\n",
    "        teacher_model=model,\n",
    "        student_model=smaller_model,\n",
    "        dataset=dataset,\n",
    "        optimizer=distill_optimizer,\n",
    "        epochs=5,\n",
    "        max_seq_len=50\n",
    "    )\n",
    "\n",
    "    print(\"Distillation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
